{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Mikita Sazanovich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import pylib\n",
    "import imlib\n",
    "import tf2lib\n",
    "import tf2gan\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import data\n",
    "import module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetConfig(object):\n",
    "  def __init__(self, config):\n",
    "    with open(config, 'r') as stream:\n",
    "      docs = yaml.load_all(stream)\n",
    "      for doc in docs:\n",
    "        for k, v in doc.items():\n",
    "          if k == 'train':\n",
    "            for k1, v1 in v.items():\n",
    "              cmd = 'self.' + k1 + '=' + repr(v1)\n",
    "              print(cmd)\n",
    "              exec(cmd)\n",
    "\n",
    "def parse_args(args):\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--test_checkpoinst_dir', type=str)  # the checkpoints to use for the testing\n",
    "  parser.add_argument('--test_only', action='store_true')  # whether to run the testing stage only\n",
    "  parsed_args = parser.parse_args(args)\n",
    "  return parsed_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.log_iterations=100\n",
      "self.image_display_iterations=250\n",
      "self.image_save_iterations=2500\n",
      "self.checkpoint_save_iterations=5000\n",
      "self.hyperparameters={'trainer': 'Trainer', 'lr': 0.0001, 'll_direct_link_w': 100, 'kl_direct_link_w': 0.1, 'll_cycle_link_w': 100, 'kl_cycle_link_w': 0.1, 'gan_w': 10, 'batch_size': 64, 'max_iterations': 80000, 'seed': 30, 'gen': {'ch': 64, 'input_dim': 3, 'n_enc_front_blk': 3, 'n_enc_res_blk': 3, 'n_enc_shared_blk': 1, 'n_dec_shared_blk': 1, 'n_dec_res_blk': 3, 'n_dec_front_blk': 3, 'res_dropout_ratio': 0.5}, 'dis': {'ch': 64, 'input_dim': 3, 'n_layer': 4}}\n",
      "self.datasets={'general': {'datasets_dir': 'datasets/duckiebot', 'load_size_width': 64, 'load_size_height': 32, 'crop_size_width': 64, 'crop_size_height': 32}, 'train_a': {'dataset_name': 'trainA', 'filter': '*.png'}, 'train_b': {'dataset_name': 'trainB', 'filter': '*.png'}, 'test_a': {'dataset_name': 'testA', 'filter': '*.png'}, 'test_b': {'dataset_name': 'testB', 'filter': '*.png'}}\n",
      "Namespace(test_checkpoinst_dir=None, test_only=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerogerc/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "config = NetConfig('exps/unit/duckietown.yaml')\n",
    "args = parse_args(\n",
    "    args=[]\n",
    "#   args=[\n",
    "#   '--test_checkpoinst_dir=/home/zerogerc/msazanovich/sim2real/duckietown/output/unit-epochs-50-20200520003002/checkpoints',\n",
    "#   '--test_only']\n",
    ")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(config.hyperparameters['seed'])\n",
    "np.random.seed(config.hyperparameters['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2DPadded(filters, kernel_size, strides, padding):\n",
    "  layers = []\n",
    "  layers.append(\n",
    "    tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same'))\n",
    "  return tf.keras.Sequential(layers=layers)\n",
    "\n",
    "def Conv2DTransposePadded(filters, kernel_size, strides, padding, output_padding):\n",
    "  layers = []\n",
    "  layers.append(\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "      filters=filters, kernel_size=kernel_size, strides=strides, padding='same', output_padding=output_padding))\n",
    "  return tf.keras.Sequential(layers=layers)\n",
    "\n",
    "def LeakyReLUConv2D(input_filters, output_filters, kernel_size, strides, padding):\n",
    "  layers = []\n",
    "  layers.append(Conv2DPadded(output_filters, kernel_size, strides, padding))\n",
    "  layers.append(tf.keras.layers.LeakyReLU())  # TODO(sazanovich): alpha = 0.3 while in torch it is 0.01\n",
    "  return tf.keras.Sequential(layers=layers)\n",
    "\n",
    "def LeakyReLUConv2DTranspose(input_filters, output_filters, kernel_size, strides, padding, output_padding):\n",
    "  layers = []\n",
    "  layers.append(Conv2DTransposePadded(output_filters, kernel_size, strides, padding, output_padding))\n",
    "  layers.append(tf.keras.layers.LeakyReLU())  # TODO(sazanovich): alpha = 0.3 while in torch it is 0.01\n",
    "  return tf.keras.Sequential(layers=layers)\n",
    "\n",
    "def Conv3x3(inplanes, outplanes, strides=1):\n",
    "  return Conv2DPadded(outplanes, kernel_size=3, strides=strides, padding=1)\n",
    "\n",
    "def INSResBlock(inplanes, planes, strides=1, dropout=0.0):  \n",
    "  layers = []\n",
    "  layers += [Conv3x3(inplanes, planes, strides)]\n",
    "  layers += [tfa.layers.InstanceNormalization()]\n",
    "  layers += [tf.keras.layers.ReLU()]\n",
    "  layers += [Conv3x3(planes, planes)]\n",
    "  layers += [tfa.layers.InstanceNormalization()]\n",
    "  if dropout > 0:\n",
    "    layers += [tf.keras.layers.Dropout(rate=dropout)]\n",
    "  block = tf.keras.Sequential(layers=layers)\n",
    "  \n",
    "  input_shape = (None, None, inplanes)\n",
    "  inputs = tf.keras.Input(shape=input_shape)\n",
    "  outputs = inputs + block(inputs)\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super(Encoder, self).__init__()\n",
    "    input_dim = params['input_dim']\n",
    "    ch = params['ch']\n",
    "    n_enc_front_blk = params['n_enc_front_blk']\n",
    "    n_enc_res_blk = params['n_enc_res_blk']\n",
    "    n_enc_shared_blk = params['n_enc_shared_blk']\n",
    "    n_dec_shared_blk = params['n_dec_shared_blk']\n",
    "    n_dec_res_blk = params['n_dec_res_blk']\n",
    "    n_dec_front_blk = params['n_dec_front_blk']\n",
    "    res_dropout_ratio = params.get('res_dropout_ratio', 0.0)\n",
    "    \n",
    "    # Convolutional front-end\n",
    "    layers = []\n",
    "    layers += [LeakyReLUConv2D(input_dim, ch, kernel_size=7, strides=1, padding=3)]\n",
    "    tch = ch\n",
    "    for i in range(1, n_enc_front_blk):\n",
    "      layers += [LeakyReLUConv2D(tch, tch * 2, kernel_size=3, strides=2, padding=1)]\n",
    "      tch *= 2\n",
    "    # Residual-block back-end\n",
    "    for i in range(0, n_enc_res_blk):\n",
    "      layers += [INSResBlock(tch, tch, dropout=res_dropout_ratio)]\n",
    "    self.model = tf.keras.Sequential(layers)\n",
    "    \n",
    "  def __call__(self, inputs):\n",
    "    return self.model(inputs)\n",
    "\n",
    "class EncoderShared(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super(EncoderShared, self).__init__()\n",
    "    input_dim = params['input_dim']\n",
    "    ch = params['ch']\n",
    "    n_enc_front_blk = params['n_enc_front_blk']\n",
    "    n_enc_res_blk = params['n_enc_res_blk']\n",
    "    n_enc_shared_blk = params['n_enc_shared_blk']\n",
    "    n_dec_shared_blk = params['n_dec_shared_blk']\n",
    "    n_dec_res_blk = params['n_dec_res_blk']\n",
    "    n_dec_front_blk = params['n_dec_front_blk']\n",
    "    res_dropout_ratio = params.get('res_dropout_ratio', 0.0)\n",
    "    \n",
    "    # Shared residual-blocks\n",
    "    layers = []\n",
    "    tch = ch * 2**(n_enc_front_blk-1)\n",
    "    for i in range(0, n_enc_shared_blk):\n",
    "      layers += [INSResBlock(tch, tch, dropout=res_dropout_ratio)]\n",
    "    layers += [tf.keras.layers.GaussianNoise(stddev=1.0)]\n",
    "    self.model = tf.keras.Sequential(layers)\n",
    "    \n",
    "  def __call__(self, inputs):\n",
    "    return self.model(inputs)\n",
    "    \n",
    "class DecoderShared(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super(DecoderShared, self).__init__()\n",
    "    input_dim = params['input_dim']\n",
    "    ch = params['ch']\n",
    "    n_enc_front_blk = params['n_enc_front_blk']\n",
    "    n_enc_res_blk = params['n_enc_res_blk']\n",
    "    n_enc_shared_blk = params['n_enc_shared_blk']\n",
    "    n_dec_shared_blk = params['n_dec_shared_blk']\n",
    "    n_dec_res_blk = params['n_dec_res_blk']\n",
    "    n_dec_front_blk = params['n_dec_front_blk']\n",
    "    res_dropout_ratio = params.get('res_dropout_ratio', 0.0)\n",
    "    \n",
    "    # Shared residual-blocks\n",
    "    layers = []\n",
    "    tch = ch * 2**(n_enc_front_blk-1)\n",
    "    for i in range(0, n_dec_shared_blk):\n",
    "      layers += [INSResBlock(tch, tch, dropout=res_dropout_ratio)]\n",
    "    self.model = tf.keras.Sequential(layers)\n",
    "    \n",
    "  def __call__(self, inputs):\n",
    "    return self.model(inputs)\n",
    "  \n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super(Decoder, self).__init__()\n",
    "    input_dim = params['input_dim']\n",
    "    ch = params['ch']\n",
    "    n_enc_front_blk = params['n_enc_front_blk']\n",
    "    n_enc_res_blk = params['n_enc_res_blk']\n",
    "    n_enc_shared_blk = params['n_enc_shared_blk']\n",
    "    n_dec_shared_blk = params['n_dec_shared_blk']\n",
    "    n_dec_res_blk = params['n_dec_res_blk']\n",
    "    n_dec_front_blk = params['n_dec_front_blk']\n",
    "    res_dropout_ratio = params.get('res_dropout_ratio', 0.0)\n",
    "    \n",
    "    # Residual-block front-end\n",
    "    layers = []\n",
    "    tch = ch * 2**(n_enc_front_blk-1)\n",
    "    for i in range(0, n_dec_res_blk):\n",
    "      layers += [INSResBlock(tch, tch, dropout=res_dropout_ratio)]\n",
    "    # Convolutional back-end\n",
    "    for i in range(0, n_dec_front_blk-1):\n",
    "      layers += [LeakyReLUConv2DTranspose(tch, tch//2, kernel_size=3, strides=2, padding=1, output_padding=1)]\n",
    "      tch = tch//2\n",
    "    layers += [Conv2DTransposePadded(filters=input_dim, kernel_size=1, strides=1, padding=0, output_padding=0)]\n",
    "    layers += [tf.keras.layers.Activation(tf.keras.activations.tanh)]\n",
    "    self.model = tf.keras.Sequential(layers)\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    return self.model(inputs)\n",
    "  \n",
    "class Discriminator(tf.keras.Model):\n",
    "  def __init__(self, params):\n",
    "    super(Discriminator, self).__init__()\n",
    "    ch = params['ch']\n",
    "    input_dim = params['input_dim']\n",
    "    n_layer = params['n_layer']\n",
    "    \n",
    "    model = []\n",
    "    model += [LeakyReLUConv2D(input_dim, ch, kernel_size=3, strides=2, padding=1)]\n",
    "    tch = ch\n",
    "    for i in range(1, n_layer):\n",
    "      model += [LeakyReLUConv2D(tch, tch * 2, kernel_size=3, strides=2, padding=1)]\n",
    "      tch *= 2\n",
    "    model += [tf.keras.layers.Conv2D(1, kernel_size=1, strides=1)]\n",
    "    self.model = tf.keras.Sequential(layers=model)\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    out = self.model(inputs)\n",
    "    out = tf.reshape(out, [-1])\n",
    "    return [out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_hyperparameters = config.hyperparameters['gen']\n",
    "encoder_a = Encoder(gen_hyperparameters)\n",
    "encoder_b = Encoder(gen_hyperparameters)\n",
    "encoder_shared = EncoderShared(gen_hyperparameters)\n",
    "decoder_shared = DecoderShared(gen_hyperparameters)\n",
    "decoder_a = Decoder(gen_hyperparameters)\n",
    "decoder_b = Decoder(gen_hyperparameters)\n",
    "\n",
    "dis_hyperparameters = config.hyperparameters['dis']\n",
    "dis_a = Discriminator(dis_hyperparameters)\n",
    "dis_b = Discriminator(dis_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encoder_decoder_ab_abab(x_a, x_b):\n",
    "  encoded_a = encoder_a(x_a)\n",
    "  encoded_b = encoder_b(x_b)\n",
    "  encoded_ab = tf.concat((encoded_a, encoded_b), axis=0)\n",
    "  encoded_shared = encoder_shared(encoded_ab)\n",
    "  decoded_shared = decoder_shared(encoded_shared)\n",
    "  decoded_a = decoder_a(decoded_shared)\n",
    "  decoded_b = decoder_b(decoded_shared)\n",
    "  x_aa, x_ba = tf.split(decoded_a, num_or_size_splits=2, axis=0)\n",
    "  x_ab, x_bb = tf.split(decoded_b, num_or_size_splits=2, axis=0)\n",
    "  return x_aa, x_ba, x_ab, x_bb, encoded_shared\n",
    "\n",
    "@tf.function\n",
    "def encoder_decoder_a_b(x_a):\n",
    "  encoded_a = encoder_a(x_a)\n",
    "  encoded_shared = encoder_shared(encoded_a)\n",
    "  decoded_shared = decoder_shared(encoded_shared)\n",
    "  decoded_b = decoder_b(decoded_shared)\n",
    "  return decoded_b, encoded_shared\n",
    "\n",
    "@tf.function\n",
    "def encoder_decoder_b_a(x_b):\n",
    "  encoded_b = encoder_b(x_b)\n",
    "  encoded_shared = encoder_shared(encoded_b)\n",
    "  decoded_shared = decoder_shared(encoded_shared)\n",
    "  decoded_a = decoder_a(decoded_shared)\n",
    "  return decoded_a, encoded_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _compute_true_acc(predictions):\n",
    "  predictions_true = tf.greater(predictions, 0.5)\n",
    "  predictions_true = tf.cast(predictions_true, predictions.dtype)\n",
    "  return tf.reduce_sum(predictions_true) / tf.size(predictions_true, out_type=predictions.dtype)\n",
    "\n",
    "@tf.function\n",
    "def _compute_fake_acc(predictions):\n",
    "  predictions_fake = tf.less(predictions, 0.5)\n",
    "  predictions_fake = tf.cast(predictions_fake, predictions.dtype)\n",
    "  return tf.reduce_sum(predictions_fake) / tf.size(predictions_fake, out_type=predictions.dtype)\n",
    "\n",
    "@tf.function\n",
    "def _compute_kl(mu):\n",
    "  mu_2 = tf.pow(mu, 2)\n",
    "  encoding_loss = tf.reduce_mean(mu_2)\n",
    "  return encoding_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "  def __init__(self, hyperparameters):\n",
    "    super(Trainer, self).__init__()\n",
    "    self.hyperparameters = hyperparameters\n",
    "    lr = self.hyperparameters['lr']\n",
    "    self.enc_dec_opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.999, decay=0.0001)\n",
    "    self.discrim_opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.5, beta_2=0.999, decay=0.0001)\n",
    "    self.dis_loss_criterion = tf.keras.losses.BinaryCrossentropy()\n",
    "    self.ll_loss_criterion_a = tf.keras.losses.MeanAbsoluteError()\n",
    "    self.ll_loss_criterion_b = tf.keras.losses.MeanAbsoluteError()\n",
    "  \n",
    "  @tf.function\n",
    "  def train_step(self, images_a, images_b):\n",
    "    D_loss_dict = self.dis_update(images_a, images_b)\n",
    "    G_images, G_loss_dict = self.enc_dec_update(images_a, images_b)\n",
    "    return G_images, G_loss_dict, D_loss_dict\n",
    "  \n",
    "  @tf.function\n",
    "  def dis_update(self, images_a, images_b):\n",
    "    with tf.GradientTape() as t:\n",
    "      x_aa, x_ba, x_ab, x_bb, shared = encoder_decoder_ab_abab(images_a, images_b)\n",
    "      data_a = tf.concat((images_a, x_ba), axis=0)\n",
    "      data_b = tf.concat((images_b, x_ab), axis=0)\n",
    "      res_a = dis_a(data_a)\n",
    "      res_b = dis_b(data_b)\n",
    "      for it, (this_a, this_b) in enumerate(zip(res_a, res_b)):\n",
    "        out_a = tf.keras.activations.sigmoid(this_a)\n",
    "        out_b = tf.keras.activations.sigmoid(this_b)\n",
    "        out_true_a, out_fake_a = tf.split(out_a, num_or_size_splits=2, axis=0)\n",
    "        out_true_b, out_fake_b = tf.split(out_b, num_or_size_splits=2, axis=0)\n",
    "        out_true_n = out_true_a.shape[0]\n",
    "        out_fake_n = out_fake_a.shape[0]\n",
    "        all1 = tf.ones([out_true_n])\n",
    "        all0 = tf.zeros([out_fake_n])\n",
    "        ad_true_loss_a = self.dis_loss_criterion(y_true=all1, y_pred=out_true_a)\n",
    "        ad_true_loss_b = self.dis_loss_criterion(y_true=all1, y_pred=out_true_b)\n",
    "        ad_fake_loss_a = self.dis_loss_criterion(y_true=all0, y_pred=out_fake_a)\n",
    "        ad_fake_loss_b = self.dis_loss_criterion(y_true=all0, y_pred=out_fake_b)\n",
    "        if it==0:\n",
    "          ad_loss_a = ad_true_loss_a + ad_fake_loss_a\n",
    "          ad_loss_b = ad_true_loss_b + ad_fake_loss_b\n",
    "        else:\n",
    "          ad_loss_a += ad_true_loss_a + ad_fake_loss_a\n",
    "          ad_loss_b += ad_true_loss_b + ad_fake_loss_b\n",
    "      loss = self.hyperparameters['gan_w'] * (ad_loss_a + ad_loss_b)\n",
    "\n",
    "    variables = []\n",
    "    for model in [dis_a, dis_b]:\n",
    "      variables.extend(model.trainable_variables)\n",
    "    grads = t.gradient(loss, variables)\n",
    "    self.discrim_opt.apply_gradients(zip(grads, variables))\n",
    "    \n",
    "    true_a_acc_batch = _compute_true_acc(out_true_a)\n",
    "    true_b_acc_batch = _compute_true_acc(out_true_b)\n",
    "    fake_a_acc_batch = _compute_fake_acc(out_fake_a)\n",
    "    fake_b_acc_batch = _compute_fake_acc(out_fake_b)\n",
    "    \n",
    "    D_loss_dict = {\n",
    "      'true_a_acc_batch': true_a_acc_batch,\n",
    "      'true_b_acc_batch': true_b_acc_batch,\n",
    "      'fake_a_acc_batch': fake_a_acc_batch,\n",
    "      'fake_b_acc_batch': fake_b_acc_batch,\n",
    "      'loss': loss,\n",
    "    }\n",
    "    return D_loss_dict\n",
    "\n",
    "  @tf.function\n",
    "  def enc_dec_update(self, images_a, images_b):\n",
    "    with tf.GradientTape() as t:\n",
    "      x_aa, x_ba, x_ab, x_bb, shared = encoder_decoder_ab_abab(images_a, images_b)\n",
    "      x_bab, shared_bab = encoder_decoder_a_b(x_ba)\n",
    "      x_aba, shared_aba = encoder_decoder_b_a(x_ab)\n",
    "      outs_a = dis_a(x_ba)\n",
    "      outs_b = dis_b(x_ab)\n",
    "      for it, (out_a, out_b) in enumerate(zip(outs_a, outs_b)):\n",
    "        outputs_a = tf.keras.activations.sigmoid(out_a)\n",
    "        outputs_b = tf.keras.activations.sigmoid(out_b)\n",
    "        outputs_n = outputs_a.shape[0]\n",
    "        all_ones = tf.ones([outputs_n])\n",
    "        ad_loss_a_add = self.dis_loss_criterion(y_true=all_ones, y_pred=outputs_a)\n",
    "        ad_loss_b_add = self.dis_loss_criterion(y_true=all_ones, y_pred=outputs_b)\n",
    "        if it==0:\n",
    "          ad_loss_a = ad_loss_a_add\n",
    "          ad_loss_b = ad_loss_b_add\n",
    "        else:\n",
    "          ad_loss_a += ad_loss_a_add\n",
    "          ad_loss_b += ad_loss_b_add\n",
    "\n",
    "      enc_loss = _compute_kl(shared)\n",
    "      enc_bab_loss = _compute_kl(shared_bab)\n",
    "      enc_aba_loss = _compute_kl(shared_aba)\n",
    "      ll_loss_a = self.ll_loss_criterion_a(y_true=images_a, y_pred=x_aa)\n",
    "      ll_loss_b = self.ll_loss_criterion_b(y_true=images_b, y_pred=x_bb)\n",
    "      ll_loss_aba = self.ll_loss_criterion_a(y_true=images_a, y_pred=x_aba)\n",
    "      ll_loss_bab = self.ll_loss_criterion_b(y_true=images_b, y_pred=x_bab)\n",
    "      loss = (\n",
    "        self.hyperparameters['gan_w'] * (ad_loss_a + ad_loss_b)\n",
    "        + self.hyperparameters['ll_direct_link_w'] * (ll_loss_a + ll_loss_b)\n",
    "        + self.hyperparameters['ll_cycle_link_w'] * (ll_loss_aba + ll_loss_bab)\n",
    "        + self.hyperparameters['kl_direct_link_w'] * (enc_loss + enc_loss)\n",
    "        + self.hyperparameters['kl_cycle_link_w'] * (enc_bab_loss + enc_aba_loss))\n",
    "\n",
    "    variables = []\n",
    "    for model in [\n",
    "        encoder_a, encoder_b, encoder_shared, decoder_shared, decoder_a, decoder_b]:\n",
    "      variables.extend(model.trainable_variables)\n",
    "    grads = t.gradient(loss, variables)\n",
    "    self.enc_dec_opt.apply_gradients(zip(grads, variables))\n",
    "    \n",
    "    G_images = [x_aa, x_ba, x_ab, x_bb, x_aba, x_bab]\n",
    "    G_loss_dict = {\n",
    "      'enc_loss': enc_loss,\n",
    "      'enc_bab_loss': enc_bab_loss,\n",
    "      'enc_aba_loss': enc_aba_loss,\n",
    "      'ad_loss_a': ad_loss_a,\n",
    "      'ad_loss_b': ad_loss_b,\n",
    "      'll_loss_a': ll_loss_a,\n",
    "      'll_loss_b': ll_loss_b,\n",
    "      'll_loss_aba': ll_loss_aba,\n",
    "      'll_loss_bab': ll_loss_bab,\n",
    "      'loss': loss,\n",
    "    }\n",
    "    return G_images, G_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config.hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_a_paths) = 64000\n",
      "len(train_b_paths) = 70342\n",
      "1099 batches in AB train\n",
      "len(test_a_paths) = 64000\n",
      "len(test_b_paths) = 70342\n",
      "1099 batches in AB test\n"
     ]
    }
   ],
   "source": [
    "def create_datasets(config_datasets, batch_size):\n",
    "  datasets_dir = config_datasets['general']['datasets_dir']\n",
    "  load_size_width = config_datasets['general']['load_size_width']\n",
    "  load_size_height = config_datasets['general']['load_size_height']\n",
    "  crop_size_width = config_datasets['general']['crop_size_width']\n",
    "  crop_size_height = config_datasets['general']['crop_size_height']\n",
    "  \n",
    "  load_size = [load_size_height, load_size_width]\n",
    "  crop_size = [crop_size_height, crop_size_width]\n",
    "  \n",
    "  config_train_a = config_datasets['train_a']\n",
    "  config_train_b = config_datasets['train_b']\n",
    "  train_a_paths = pylib.glob(os.path.join(datasets_dir, config_train_a['dataset_name']), config_train_a['filter'])\n",
    "  train_b_paths = pylib.glob(os.path.join(datasets_dir, config_train_b['dataset_name']), config_train_b['filter'])\n",
    "  ab_train_dataset, ab_train_length = data.make_zip_dataset(\n",
    "    train_a_paths,\n",
    "    train_b_paths,\n",
    "    batch_size,\n",
    "    load_size,\n",
    "    crop_size,\n",
    "    training=True,\n",
    "    repeat=False)\n",
    "  print(f'len(train_a_paths) = {len(train_a_paths)}')\n",
    "  print(f'len(train_b_paths) = {len(train_b_paths)}')\n",
    "  print(f'{ab_train_length} batches in AB train')\n",
    "\n",
    "  config_test_a = config_datasets['test_a']\n",
    "  config_test_b = config_datasets['test_b']\n",
    "  test_a_paths = pylib.glob(os.path.join(datasets_dir, config_test_a['dataset_name']), config_test_a['filter'])\n",
    "  test_b_paths = pylib.glob(os.path.join(datasets_dir, config_test_b['dataset_name']), config_test_b['filter'])\n",
    "  ab_test_dataset, ab_test_length = data.make_zip_dataset(\n",
    "    test_a_paths,\n",
    "    test_b_paths,\n",
    "    batch_size,\n",
    "    load_size,\n",
    "    crop_size,\n",
    "    training=False,\n",
    "    repeat=True)\n",
    "  print(f'len(test_a_paths) = {len(test_a_paths)}')\n",
    "  print(f'len(test_b_paths) = {len(test_b_paths)}')\n",
    "  print(f'{ab_test_length} batches in AB test')\n",
    "  \n",
    "  return ab_train_dataset, ab_train_length, ab_test_dataset, ab_test_length\n",
    "\n",
    "ab_train_dataset, ab_train_length, ab_test_dataset, ab_test_length = create_datasets(\n",
    "  config.datasets, config.hyperparameters['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint specified (save_path=None); nothing is being restored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10477743f0b348519b8a5724b626a237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the iterator...\n",
      "Resetting the iterator...\n",
      "Resetting the iterator...\n",
      "Resetting the iterator...\n",
      "Resetting the iterator...\n",
      "Resetting the iterator...\n",
      "Resetting the iterator...\n",
      "Resetting the iterator...\n",
      "Resetting the iterator...\n"
     ]
    }
   ],
   "source": [
    "# Output directories\n",
    "output_dir_base = '/home/zerogerc/msazanovich/sim2real/duckietown/output'\n",
    "output_dir_name = f'unit-{time.strftime(\"%Y%m%d%H%M%S\")}'\n",
    "output_dir = os.path.join(output_dir_base, output_dir_name)\n",
    "os.makedirs(output_dir, exist_ok=False)\n",
    "\n",
    "samples_dir = os.path.join(output_dir, 'samples')\n",
    "summaries_dir = os.path.join(output_dir, 'summaries')\n",
    "os.makedirs(samples_dir, exist_ok=False)\n",
    "os.makedirs(summaries_dir, exist_ok=False)\n",
    "\n",
    "# Checkpointing setup\n",
    "checkpoints_dir = os.path.join(output_dir, 'checkpoints')\n",
    "checkpoint_dict = {\n",
    "    'encoder_a': encoder_a,\n",
    "    'encoder_b': encoder_b,\n",
    "    'encoder_shared': encoder_shared,\n",
    "    'decoder_shared': decoder_shared,\n",
    "    'decoder_a': decoder_a,\n",
    "    'decoder_b': decoder_b,\n",
    "    'dis_a': dis_a,\n",
    "    'dis_b': dis_b,\n",
    "    'enc_dec_opt': trainer.enc_dec_opt,\n",
    "    'discrim_opt': trainer.discrim_opt,\n",
    "}\n",
    "checkpoint = tf2lib.Checkpoint(checkpoint_dict, checkpoints_dir, max_to_keep=5)\n",
    "try:  # Restore checkpoint\n",
    "  checkpoint.restore().assert_existing_objects_matched()\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(summaries_dir)\n",
    "  \n",
    "def train():\n",
    "  max_iterations = config.hyperparameters['max_iterations']\n",
    "  dataset_iter = iter(ab_train_dataset)\n",
    "  for iterations in tqdm.tqdm(range(max_iterations)):\n",
    "    try:\n",
    "      images_a, images_b = next(dataset_iter)\n",
    "    except StopIteration:\n",
    "      print('Resetting the iterator...')\n",
    "      dataset_iter = iter(ab_train_dataset)\n",
    "      images_a, images_b = next(dataset_iter)\n",
    "\n",
    "    # Training ops\n",
    "    G_images, G_loss_dict, D_loss_dict = trainer.train_step(images_a, images_b)\n",
    "\n",
    "    # Logging ops\n",
    "    if (iterations + 1) % config.log_iterations == 0:\n",
    "      with train_summary_writer.as_default():\n",
    "        tf2lib.summary(D_loss_dict, step=iterations, name='discriminator')\n",
    "        tf2lib.summary(G_loss_dict, step=iterations, name='generator')\n",
    "    # Displaying ops\n",
    "    if (iterations + 1) % config.image_save_iterations == 0:\n",
    "      img_filename = os.path.join(samples_dir, f'train_{iterations + 1}.jpg')\n",
    "    elif (iterations + 1) % config.image_display_iterations == 0:\n",
    "      img_filename = os.path.join(samples_dir, f'train.jpg')\n",
    "    else:\n",
    "      img_filename = None\n",
    "    if img_filename:\n",
    "      img = imlib.immerge(np.concatenate([images_a, images_b] + G_images, axis=0), n_rows=8)\n",
    "      imlib.imwrite(img, img_filename)\n",
    "    # Checkpointing ops\n",
    "    if (iterations + 1) % config.checkpoint_save_iterations == 0 or iterations + 1 == max_iterations:\n",
    "      checkpoint.save(iterations + 1)\n",
    "      \n",
    "if not args.test_only:\n",
    "  train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore from the checkpoint\n",
    "test_checkpoinst_dir = args.test_checkpoinst_dir if args.test_checkpoinst_dir else checkpoints_dir\n",
    "test_latest_checkpoint = tf.train.latest_checkpoint(test_checkpoinst_dir)\n",
    "print(test_latest_checkpoint)\n",
    "\n",
    "test_checkpoint = tf.train.Checkpoint(**checkpoint_dict)\n",
    "test_checkpoint.restore(test_latest_checkpoint).assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCHES_TO_SAVE = 10\n",
    "\n",
    "test_dataset_iter = iter(ab_test_dataset)\n",
    "for iterations in tqdm.tqdm(range(ab_test_length)):\n",
    "  try:\n",
    "    images_a, images_b = next(test_dataset_iter)\n",
    "  except StopIteration:\n",
    "    break\n",
    "  \n",
    "  # Inference ops\n",
    "  x_aa, x_ba, x_ab, x_bb, shared = encoder_decoder_ab_abab(images_a, images_b)\n",
    "  x_bab, shared_bab = encoder_decoder_a_b(x_ba)\n",
    "  x_aba, shared_aba = encoder_decoder_b_a(x_ab)\n",
    "  G_images = [x_aa, x_ba, x_ab, x_bb, x_aba, x_bab]\n",
    "  \n",
    "  # Displaying ops\n",
    "  if (iterations + 1) % (ab_test_length // TEST_BATCHES_TO_SAVE) == 0:\n",
    "    img_filename = os.path.join(samples_dir, f'test_{iterations + 1}.jpg')\n",
    "    img = imlib.immerge(np.concatenate([images_a, images_b] + G_images, axis=0), n_rows=8)\n",
    "    imlib.imwrite(img, img_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "250px",
    "width": "164px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}