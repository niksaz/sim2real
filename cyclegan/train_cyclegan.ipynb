{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Mikita Sazanovich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import functools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pylib\n",
    "import imlib\n",
    "import tf2lib\n",
    "import tf2gan\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import data\n",
    "import cyclegan.module as module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--dataset', default='horse2zebra')\n",
    "  parser.add_argument('--datasets_dir', default='datasets')\n",
    "  parser.add_argument('--load_size_width', type=int, default=286)  # loaded images are resized to this width\n",
    "  parser.add_argument('--load_size_height', type=int, default=286)  # and this height\n",
    "  parser.add_argument('--crop_size_width', type=int, default=256)  # then cropped to this width\n",
    "  parser.add_argument('--crop_size_height', type=int, default=256)  # and this height\n",
    "  parser.add_argument('--batch_size', type=int, default=1)\n",
    "  parser.add_argument('--epochs', type=int, default=200)\n",
    "  parser.add_argument('--epoch_decay', type=int, default=100)  # epoch to start decaying learning rate\n",
    "  parser.add_argument('--lr', type=float, default=0.0002)\n",
    "  parser.add_argument('--beta_1', type=float, default=0.5)\n",
    "  parser.add_argument('--adversarial_loss_mode', default='lsgan', choices=['gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan'])\n",
    "  parser.add_argument('--gradient_penalty_mode', default='none', choices=['none', 'dragan', 'wgan-gp'])\n",
    "  parser.add_argument('--gradient_penalty_weight', type=float, default=10.0)\n",
    "  parser.add_argument('--cycle_loss_weight', type=float, default=10.0)\n",
    "  parser.add_argument('--identity_loss_weight', type=float, default=0.0)\n",
    "  parser.add_argument('--pool_size', type=int, default=50)  # pool size to store fake samples\n",
    "  parsed_args = parser.parse_args(args)\n",
    "  return parsed_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(args=[\n",
    "  \"--dataset=duckiebot\",\n",
    "  \"--batch_size=64\",\n",
    "  \"--epochs=2\",\n",
    "  \"--epoch_decay=1\",\n",
    "  \"--load_size_width=64\",\n",
    "  \"--load_size_height=32\",\n",
    "  \"--crop_size_width=64\",\n",
    "  \"--crop_size_height=32\"])\n",
    "output_dir = os.path.join('output', args.dataset)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "settings_path = os.path.join(output_dir, 'settings.json')\n",
    "pylib.args_to_json(settings_path, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(A_img_paths) = 64000\n",
      "len(B_img_paths) = 70342\n"
     ]
    }
   ],
   "source": [
    "A_img_paths = pylib.glob(os.path.join(args.datasets_dir, args.dataset, 'trainA'), '*.png')\n",
    "B_img_paths = pylib.glob(os.path.join(args.datasets_dir, args.dataset, 'trainB'), '*.png')\n",
    "print(f'len(A_img_paths) = {len(A_img_paths)}')\n",
    "print(f'len(B_img_paths) = {len(B_img_paths)}')\n",
    "\n",
    "load_size = [args.load_size_height, args.load_size_width]\n",
    "crop_size = [args.crop_size_height, args.crop_size_width]\n",
    "A_B_dataset, len_dataset = data.make_zip_dataset(\n",
    "  A_img_paths, B_img_paths, args.batch_size, load_size, crop_size, training=True, repeat=False)\n",
    "\n",
    "A2B_pool = data.ItemPool(args.pool_size)\n",
    "B2A_pool = data.ItemPool(args.pool_size)\n",
    "\n",
    "A_img_paths_test = pylib.glob(os.path.join(args.datasets_dir, args.dataset, 'testA'), '*.png')\n",
    "B_img_paths_test = pylib.glob(os.path.join(args.datasets_dir, args.dataset, 'testB'), '*.png')\n",
    "A_B_dataset_test, _ = data.make_zip_dataset(\n",
    "  A_img_paths_test, B_img_paths_test, args.batch_size, load_size, crop_size, training=False, repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_shape = crop_size + [3]  # [args.crop_size_height, args.crop_size_width, 3]\n",
    "\n",
    "G_A2B = module.ResnetGenerator(input_shape=model_input_shape, n_blocks=6)\n",
    "G_B2A = module.ResnetGenerator(input_shape=model_input_shape, n_blocks=6)\n",
    "\n",
    "D_A = module.ConvDiscriminator(input_shape=model_input_shape)\n",
    "D_B = module.ConvDiscriminator(input_shape=model_input_shape)\n",
    "\n",
    "d_loss_fn, g_loss_fn = tf2gan.get_adversarial_losses_fn(args.adversarial_loss_mode)\n",
    "cycle_loss_fn = tf.losses.MeanAbsoluteError()\n",
    "identity_loss_fn = tf.losses.MeanAbsoluteError()\n",
    "\n",
    "G_lr_scheduler = module.LinearDecay(args.lr, args.epochs * len_dataset, args.epoch_decay * len_dataset)\n",
    "D_lr_scheduler = module.LinearDecay(args.lr, args.epochs * len_dataset, args.epoch_decay * len_dataset)\n",
    "G_optimizer = tf.keras.optimizers.Adam(learning_rate=G_lr_scheduler, beta_1=args.beta_1)\n",
    "D_optimizer = tf.keras.optimizers.Adam(learning_rate=D_lr_scheduler, beta_1=args.beta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_generators(A, B):\n",
    "  with tf.GradientTape() as t:\n",
    "    A2B = G_A2B(A, training=True)\n",
    "    B2A = G_B2A(B, training=True)\n",
    "    A2B2A = G_B2A(A2B, training=True)\n",
    "    B2A2B = G_A2B(B2A, training=True)\n",
    "    A2A = G_B2A(A, training=True)\n",
    "    B2B = G_A2B(B, training=True)\n",
    "\n",
    "    A2B_d_logits = D_B(A2B, training=True)\n",
    "    B2A_d_logits = D_A(B2A, training=True)\n",
    "\n",
    "    A2B_g_loss = g_loss_fn(A2B_d_logits)\n",
    "    B2A_g_loss = g_loss_fn(B2A_d_logits)\n",
    "    A2B2A_cycle_loss = cycle_loss_fn(A, A2B2A)\n",
    "    B2A2B_cycle_loss = cycle_loss_fn(B, B2A2B)\n",
    "    A2A_id_loss = identity_loss_fn(A, A2A)\n",
    "    B2B_id_loss = identity_loss_fn(B, B2B)\n",
    "\n",
    "    G_loss = (\n",
    "      (A2B_g_loss + B2A_g_loss)\n",
    "      + (A2B2A_cycle_loss + B2A2B_cycle_loss) * args.cycle_loss_weight\n",
    "      + (A2A_id_loss + B2B_id_loss) * args.identity_loss_weight)\n",
    "\n",
    "  G_grad = t.gradient(G_loss, G_A2B.trainable_variables + G_B2A.trainable_variables)\n",
    "  G_optimizer.apply_gradients(zip(G_grad, G_A2B.trainable_variables + G_B2A.trainable_variables))\n",
    "  \n",
    "  loss_dict = {\n",
    "    'A2B_g_loss': A2B_g_loss,\n",
    "    'B2A_g_loss': B2A_g_loss,\n",
    "    'A2B2A_cycle_loss': A2B2A_cycle_loss,\n",
    "    'B2A2B_cycle_loss': B2A2B_cycle_loss,\n",
    "    'A2A_id_loss': A2A_id_loss,\n",
    "    'B2B_id_loss': B2B_id_loss}\n",
    "\n",
    "  return A2B, B2A, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_discriminators(A, B, A2B, B2A):\n",
    "  with tf.GradientTape() as t:\n",
    "    A_d_logits = D_A(A, training=True)\n",
    "    B2A_d_logits = D_A(B2A, training=True)\n",
    "    B_d_logits = D_B(B, training=True)\n",
    "    A2B_d_logits = D_B(A2B, training=True)\n",
    "\n",
    "    A_d_loss, B2A_d_loss = d_loss_fn(A_d_logits, B2A_d_logits)\n",
    "    B_d_loss, A2B_d_loss = d_loss_fn(B_d_logits, A2B_d_logits)\n",
    "    D_A_gp = tf2gan.gradient_penalty(functools.partial(D_A, training=True), A, B2A, mode=args.gradient_penalty_mode)\n",
    "    D_B_gp = tf2gan.gradient_penalty(functools.partial(D_B, training=True), B, A2B, mode=args.gradient_penalty_mode)\n",
    "\n",
    "    D_loss = (\n",
    "      (A_d_loss + B2A_d_loss)\n",
    "      + (B_d_loss + A2B_d_loss)\n",
    "      + (D_A_gp + D_B_gp) * args.gradient_penalty_weight)\n",
    "\n",
    "  D_grad = t.gradient(D_loss, D_A.trainable_variables + D_B.trainable_variables)\n",
    "  D_optimizer.apply_gradients(zip(D_grad, D_A.trainable_variables + D_B.trainable_variables))\n",
    "\n",
    "  loss_dict = {\n",
    "    'A_d_loss': A_d_loss + B2A_d_loss,\n",
    "    'B_d_loss': B_d_loss + A2B_d_loss,\n",
    "    'D_A_gp': D_A_gp,\n",
    "    'D_B_gp': D_B_gp}\n",
    "\n",
    "  return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(A, B):\n",
    "  A2B, B2A, G_loss_dict = train_generators(A, B)\n",
    "\n",
    "  # cannot autograph `A2B_pool`\n",
    "  A2B = A2B_pool(A2B)  # or A2B = A2B_pool(A2B.numpy()), but it is much slower\n",
    "  B2A = B2A_pool(B2A)  # because of the communication between CPU and GPU\n",
    "\n",
    "  D_loss_dict = train_discriminators(A, B, A2B, B2A)\n",
    "\n",
    "  return G_loss_dict, D_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample(A, B):\n",
    "  A2B = G_A2B(A, training=False)\n",
    "  B2A = G_B2A(B, training=False)\n",
    "  A2B2A = G_B2A(A2B, training=False)\n",
    "  B2A2B = G_A2B(B2A, training=False)\n",
    "  return A2B, B2A, A2B2A, B2A2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint specified (save_path=None); nothing is being restored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2522143a668542e8acb8abc89cfa1310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch Loop', max=2.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8196229419147c883b1e872ea4e1b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Inner Epoch Loop', max=1099.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941122ca13a64128b11ff47c045bf1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Inner Epoch Loop', max=1099.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# epoch counter\n",
    "ep_cnt = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = tf2lib.Checkpoint(\n",
    "  dict(G_A2B=G_A2B, G_B2A=G_B2A, D_A=D_A, D_B=D_B, G_optimizer=G_optimizer, D_optimizer=D_optimizer, ep_cnt=ep_cnt),\n",
    "  os.path.join(output_dir, 'checkpoints'),\n",
    "  max_to_keep=5)\n",
    "try:  # restore checkpoint including the epoch counter\n",
    "  checkpoint.restore().assert_existing_objects_matched()\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "# summary\n",
    "train_summary_writer = tf.summary.create_file_writer(os.path.join(output_dir, 'summaries', 'train'))\n",
    "\n",
    "# sample\n",
    "test_iter = iter(A_B_dataset_test)\n",
    "sample_dir = os.path.join(output_dir, 'samples_training')\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# main loop\n",
    "with train_summary_writer.as_default():\n",
    "  for ep in tqdm.trange(args.epochs, desc='Epoch Loop'):\n",
    "    if ep < ep_cnt:\n",
    "      continue\n",
    "\n",
    "    # update epoch counter\n",
    "    ep_cnt.assign_add(1)\n",
    "\n",
    "    # train for an epoch\n",
    "    for A, B in tqdm.tqdm(A_B_dataset, desc='Inner Epoch Loop', total=len_dataset):\n",
    "      G_loss_dict, D_loss_dict = train_step(A, B)\n",
    "\n",
    "      # summary\n",
    "      tf2lib.summary(G_loss_dict, step=G_optimizer.iterations, name='G_losses')\n",
    "      tf2lib.summary(D_loss_dict, step=G_optimizer.iterations, name='D_losses')\n",
    "      tf2lib.summary(\n",
    "        {'learning rate': G_lr_scheduler.current_learning_rate},\n",
    "        step=G_optimizer.iterations,\n",
    "        name='learning rate')\n",
    "\n",
    "      # sample\n",
    "      if G_optimizer.iterations.numpy() % 100 == 0:\n",
    "        A, B = next(test_iter)\n",
    "        A2B, B2A, A2B2A, B2A2B = sample(A, B)\n",
    "        img = imlib.immerge(np.concatenate([A, A2B, A2B2A, B, B2A, B2A2B], axis=0), n_rows=6)\n",
    "        imlib.imwrite(img, os.path.join(sample_dir, 'iter-%09d.jpg' % G_optimizer.iterations.numpy()))\n",
    "\n",
    "    # save checkpoint\n",
    "    checkpoint.save(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "187px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}